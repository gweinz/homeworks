{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import pickle\n",
    "import timeit\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>3.0</td>\n",
       "      <td>964982400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  item_id rating  timestamp\n",
       "1       1        3    4.0  964981247\n",
       "2       1        6    4.0  964982224\n",
       "3       1       47    5.0  964983815\n",
       "4       1       50    5.0  964982931\n",
       "5       1       70    3.0  964982400"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('ml-latest-small/ratings_train.csv', names=header)\n",
    "df = df[1:]\n",
    "df['item_id'] = pd.to_numeric(df['item_id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(df,test_size=0.25)\n",
    "\n",
    "train_data = pd.DataFrame(train_data)\n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_u = len(df[\"user_id\"].unique())\n",
    "n_m = max(df[\"item_id\"].unique())\n",
    "\n",
    "R = np.zeros((n_u, n_m))\n",
    "for line in train_data.itertuples():\n",
    "    R[int(line[1])-1, int(line[2])-1] = line[3]  \n",
    "    \n",
    "T = np.zeros((n_u, n_m))\n",
    "for line in test_data.itertuples():\n",
    "    T[int(line[1])-1, int(line[2])-1] = line[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [3., 0., 0., ..., 0., 0., 0.],\n",
       "       [5., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-5c70724713dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Print how long it took\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Run took %.2f seconds\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Check performance by plotting train and test errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scoring Function: Root Mean Squared Error\n",
    "def rmse_score(R, Q, P):\n",
    "    I = R != 0  # Indicator function which is zero for missing data\n",
    "    ME = I * (R - np.dot(P, Q.T))  # Errors between real and predicted ratings\n",
    "    MSE = ME**2  \n",
    "    return np.sqrt(np.sum(MSE)/np.sum(I))  # sum of squared errors\n",
    "\n",
    "# Set parameters and initialize latent factors\n",
    "f = 20  # Number of latent factor pairs\n",
    "lmbda = 0.5 # Regularisation strength\n",
    "gamma=0.01  # Learning rate\n",
    "n_epochs = 50  # Number of loops through training data\n",
    "P = 3 * np.random.rand(n_u, f) # Latent factors for users\n",
    "Q = 3 * np.random.rand(n_m, f) # Latent factors for movies\n",
    "\n",
    "# Stochastic GD\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "users,items = R.nonzero()      \n",
    "for epoch in range(n_epochs):\n",
    "    for u, i in zip(users,items):\n",
    "        e = R[u, i] - np.dot(P[u, :], Q[i, :].T)  # Error for this observation\n",
    "        P[u, :] += gamma * ( e * Q[i, :] - lmbda * P[u, :]) # Update this user's features\n",
    "        Q[i, :] += gamma * ( e * P[u, :] - lmbda * Q[i, :])  # Update this movie's features\n",
    "    train_errors.append(rmse_score(R,Q,P)) # Training RMSE for this pass\n",
    "    test_errors.append(rmse_score(T,Q,P)) # Test RMSE for this pass\n",
    "\n",
    "# Print how long it took\n",
    "print(\"Run took %.2f seconds\" % (timeit.default_timer() - start_time))\n",
    "    \n",
    "# Check performance by plotting train and test errors\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_errors, color=\"g\", label='Training RMSE')\n",
    "ax.plot(test_errors, color=\"r\", label='Test RMSE')\n",
    "snp.labs(\"Number of Epochs\", \"RMSE\", \"Error During Stochastic GD\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
